<!DOCTYPE html>
<html lang="en"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Machine Learning Class Project
  | CS, Georgia Tech | Fall 2020: CS 4641</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->
<link rel="stylesheet" href="https://www.w3schools.com/lib/w3-colors-2018.css">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
.texts {
width: 45em;
margin-left: auto;
margin-right: auto;
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
.headings{
  margin = 0;

}
code {
  font-family: Consolas,"courier new";
  color: crimson;
  background-color: #f1f1f1;
  padding: 2px;
  margin:0;
  font-size: 105%;
}
.w3-myfont {
  font-family: "Comic Sans MS", cursive, sans-serif;
  text-align:center;
  font-size: 55px;
  font-weight: normal;
}
body{
  font-family: 'Myriad Pro', Calibri, Helvetica, Arial, sans-serif;
  font-size: 110%;
  line-height: 1.5;
  color:#373737;
  -webkit-font-smoothing: antialiased;
}
h3{
  font-weight: normal;
  font-size: 30px;
}
h2{
  text-align: center;
}
h4{
  font-size: 23px;
  font-weight: bold;
}
</style>

<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body class="w3-container w3-light-gray"; style ="margin:0;padding:0;">
  <!-- Title and Name -->
  <div class="w3-panel w3-2018-quetzal-green w3-card-2"; style ="margin:0; padding-top:20px;">
  <h1 class = "w3-myfont";>MASKIFY</h1>
  <span style="display: block;text-align:center; font-size: 16px; line-height: 3em;">Nina Chen, Sabrina Chua, Siyu Li, Yuhe Chen</span><br>
  <span style="display: block;text-align:center; font-size: 13px; line-height: 0em;">Fall 2020 CS 4641 Machine Learning: Class Project</span><br>
  <span style="display: block;text-align:center; font-size: 13px; line-height: 1.5em;">Georgia Tech</span>
  <hr>
  </div>
<div class="container">

<div class="page-header">



<!-- Project Proposal -->
<div class="texts">
<div>
<div class="w3-panel w3-teal";>
  <h2>Summary Figure</h2>
</div>
<div style="text-align: center;">
  <img style="height: 300px;" alt="" src="Maskify_Infographic.jpg">
  <p>
    FIGURE 1: infographic
  </p>
</div>
<div class="texts">
<div>
<div class="w3-panel w3-teal";>
  <h2>Introduction/Background</h2>
</div>
For our project, our objective is to develop an image classification model to detect if an individual in a picture is wearing a mask through the use of convolutional neural networks in YOLOv3 and Darknet. In order to perform this, we plan on using feature extraction in combination with other techniques, such as Principal Component Analysis (PCA).


<br><br>

Currently, there is no smart way of monitoring continuous mask wearing in public spaces. Wearing masks is only monitored upon entrance into certain facilities by human, phone, or tablet. Motorola Solutions and other Artificial Intelligence startup companies are in the process of developing face mask recognition surveillance softwares for airlines and cities.
<br><br>
Since this is a classification problem, we decided to use binary classification as we are only predicting one of two classes. The features that we will be extracting are people with masks and people without masks so that we can derive a count of people with masks out of the total people. In our dataset, we may have a few outliers such as people wearing masks that have mouths imprinted or people wearing clear masks. These represent the edge cases where if the model would be able to accurately identify these cases, then this would indicate if the accuracy and precision are high.


<div class="texts">
<div>
<div class="w3-panel w3-teal";>
  <h2>Unsupervised Learning</h2>
</div>


<h4 class="w3-text-gray";style="font-size:40;"> DATA COLLECTION AND PREPROCESSING</h4>
<div>To gather our data, we used Google Images and searched using various keywords, including &ldquo;masks&rdquo; and &ldquo;people&rdquo;. We then used an image downloader extension (Imageye) to extract all of the images at once. Once we finished compiling this, we uploaded the data onto Github, separated, and then cleaned the data into four folders: all compiled raw files, people with masks, people without masks, and groups of people with both masks and no masks. We created a large dataset (approximately 1000 images per folder) as more situations in the training set takes into account that the model may encounter, the better its performance will be. For the cleaning portion, we specifically made sure that all of the images contained in the folders were accurate so that it would not hinder model results. For instance, the people with masks folder did not contain any images of people without masks. The resulting ratio of the amount of images with people with face masks to people without face masks is approximately equal (the approximate ratio is 47:53). Following this, we uploaded the cleaned dataset from Github to Kaggle to allow us to easily import the dataset into Google Colab (Figure 2).
<br><br>
<div style="text-align: center;"><img style="height: 100px;" src="figure2.png" alt="" /> <p>FIGURE 2: Uploading Dataset from Kaggle onto Google Colab
</div></p>
<br><br>
In terms of color, the team decided to normalize RGB color parameters to between 0 and 1 (one result shown in Figure 3). We decided to do this because when training CNN,
we plan on multiplying weights and adding biases to the initial inputs in order to cause activations that we will then back-propogate with the gradients to train the model.
Therefore, the parameters need to be on the same scale.
<br><br>
<div style="text-align: center;"><img style="height: 300px;" src="figure3.png" alt="" /> <p>FIGURE 3: Image Normalization on the Color Format</div></p>
<br><br>
To ensure the steps were done correctly, we checked the mean and standard deviation values
of the input along with the minimum and maximum values (Figure 4).
The results confirmed that the pixel range was between [0, 1].
<br><br>
  <pre>
      <code>
        print(‘Mean: %.3f,  Standard Deviation: %.3f' % (mean,std))
        print(‘Min: %.3f,  Max: %.3f' % (pixels.min(), pixels.max()))
        print(‘Data Type:  %s' % (pixels.dtype())
        plt.figure()
        plt.imshow(pixels, interpolation = 'none')
      </code>
    </pre>
    <div style="text-align: center;">
   <p>FIGURE 4: Checking Output Ranges of Image Pixels</div></p>
<br><br>
Diving more into standardization of our data,
YOLOv3 reshapes images automatically prior to training, so we do not need to normalize
the shape of data. Darknet helps to resize the images for us so that the CNN works.
</div>
<br><br>


<h4 class="w3-text-gray">UNSUPERVISED LEARNING - PRINCIPLE COMPONENT ANALYSIS (PCA) </h4>
<div>
  For the unsupervised portion, we will be primarily using PCA to perform dimensionality
  reduction to reduce the number of variables in our modeling data. Specifically in our case,
  we are applying PCA on our image matrices (composed of people with masks and people without masks)
  to reduce the dimensionality to a smaller number of principal components.
  It is important to note that this is a lossy compression since we are discarding
  some of the information. Therefore, we are aiming to perform PCA while also preserving
  the most important relationships between the variables observed in the data (people masks or not),
  by attempting to choose a standardized principal component that keeps the recovered variance at 95% or higher.
  The resulting components are shown in Figure 5 below.
<br><br>
  <div style="text-align: center;"><img style="height: 300px;" src="figure5.png" alt="" /> <p>FIGURE 5: Results of Applying PCA on Images</div></p>
<br>
To do this, we first normalized our data before conducting PCA. Normalization is
important in PCA since it is a variance maximizing exercise.
It projects our original data and normalizes it to a normal distribution
onto directions which maximize the variance. This is especially important
because we later inspect elements, such as recovered variance, to decide which principle component is best.
<br><br>
After applying PCA, we assessed how much visual information we retained and visually inspected to see how good the reconstructed images are for different numbers of selected components. Ultimately, we then chose component 20, this is based on a recovered variance of approximately 95%+ (i.e., we saw 0.986, 0.993, 0.997, etc. across multiple images) to minimize loss of information.
<br><br>
We suspect that using YOLOv3 with PCA would not only standardize our data and
speed up processing based off studies showing that PCA with YOLOv3 increased the mean
average precision (mAP) by 7.3% (Figure 6). Furthermore, using PCA could also reveal representation
of other numerical measurements of several variables in a space of few dimensions, where our senses can
perceive relationships that would otherwise remain hidden in higher dimensions.
<br><br>
  <div style="text-align: center;"><img style="height: 300px;" src="figure6.png" alt="" /> <p>FIGURE 6: YOLOv3 With and Without PCA</div></p>
<br>
While PCA does not eliminate noise, it can reduce noise. Therefore, PCA could potentially denoise our dataset by removing some of the noise that could potentially exist in higher dimensions.
<br><br>


<h4 class="w3-text-gray"> UNSUPERVISED LEARNING - COLOR QUANTIZATION WITH K-MEANS</h4>
Color quantization is an important technique for image analysis that will allow us to reduce the amount of distinct colors contained within a colored image. For our approach, we developed a k-means clustering model to design the optimal color table for color quantization whereby each component within k-means represents a type of color in the palette. These colors are composed of pixels that are grouped into different components. By using this color clustering technique, we hope to produce a subset of colors that capture all the properties of our images. Then, using the small set of colors found by this clustering technique, quantization is applied to produce a simplified image in both color and shape.
<br><br>
Color quantization is an important technique for image analysis that will allow us to reduce the amount of distinct colors contained within a colored image. For our approach, we developed a k-means clustering model to design the optimal color table for color quantization whereby each component within k-means represents a type of color in the palette. These colors are composed of pixels that are grouped into different components. By using this color clustering technique, we hope to produce a subset of colors that capture all the properties of our images. Then, using the small set of colors found by this clustering technique, quantization is applied to produce a simplified image in both color and shape.
<br><br>
We decided to use an HSV-approach over RBG since we know that HSV separates the Chroma and Luma information of an image (leading to better clustering) Since these values are separated, we will be able to later construct a histogram or thresholding rules using only saturation and hue for deeper analysis. Commonly, when using RGB, it fails to determine the color and intensity variations and often two distinct colors are merged together. The comparisons of both can be observed in the figure below.
<br><br>
<div style="text-align: center;"><img style="height: 300px;" src="figure7.png" alt="" /> <p>FIGURE 7: Original Images, Segmentation using HSV Features, Segmentation using RGB Features (from top to bottom) </div></p>
<br>
To determine the number of clusters to use, we used the Elbow Method with inertia (Figure 8) on our testing data. As a result, we confirmed that the optimal number of k clusters for the HSV is 4 clusters, which was at the elbow and looked visually the best. At first, we used a clustering of k=20 which visually looked similar to the original image so it was evident that it was not the optimal k. To briefly recap, inertia is the sum of squared distances of samples to their closest cluster center. Ideally, to obtain a good clustering, we would like to have a small value of inertia and number of clusters. Since the value of inertia decreases as the amount of clusters increases, we use the elbow point in the inertia graph to determine the optimal point whereby the change in the value of inertia becomes no longer significant.
<br><br>
<div style="text-align: center;"><img style="height: 300px;" src="figure8.png" alt="" /> <p>FIGURE 8: Graph of Inertia in Relation to the Values of K</div></p>
<br>
We applied K-means clustering on an HSV scale by first taking in the RBG images, converting them into HSV, quantizing the 1-dimensional Hue space, and converting it back to RGB. More specifically, we computed the k-means clusters for the input image in the hue dimension of the HSV space. We then replaced the hue values with its nearest cluster’s hue value, while simultaneously keeping the saturation and value channels the same as the input. Finally, we converted the quantized image back to the RGB color space.
<br><br>
<div style="text-align: center;"><img style="height: 300px;" src="figure9.png" alt="" /> <p>FIGURE 9: Original Image vs Quantized Image</div></p>
<br>
<div style="text-align: center;"><img style="height: 300px;" src="figure10.png" alt="" /> <p>FIGURE 10: Original Image (Left) vs. Image with HSV and k = 4 Clusters (Right)</div></p>
<br>
To analyze our results, we computed and displayed the histograms of the images’ hue values (example shown below).
<br><br>
<div style="text-align: center;"><img style="height: 300px;" src="figure11.png" alt="" /> <p>FIGURE 11: Example of an Image Histogram with k = 4 (Left) and k = 20 (Right) Equal vs Clustered Bins</div></p>
<br>
The first histogram uses equally-spaced bins by uniformly dividing up the hue values, while the second histogram uses k cluster center memberships bins. The k clustering has all pixels belonging to one hue cluster grouped into one bin. We can see that using the clustered bins, it is much more consistent and spread out in comparison to the equally separated bins.
<br><br>

<h4 class="w3-text-gray"> UNSUPERVISED RESULTS</h4>
In summary for our unsupervised, we cleaned and prepared our dataset to begin our unsupervised learning. In order to conduct PCA, we performed color normalization on our images. Afterwards, PCA was applied and we assessed how much visual information was retained-- we ultimately chose a component of 20 since it typically yields a recovered variance of approximately 95% or higher. Conducting PCA not only allows us to denoise some of our data, but also speed up processing later on for YOLOv3 since our dataset is large. This helped speed up processing with YOLO because although there is a higher mAP for higher resolution images, it is much slower to process. Lastly, image quantization was performed to optimize the performance of our model further. Moving into supervised learning, we learned that applying the color quantization with PCA caused the pictures to be too difficult to differentiate the pixels; therefore, the final method we decided to move forward with was to apply the image standardization and compression with PCA. We included our results for the color quantization because if we would like to expand the project further, we could test our model with it or inspire other future projects to use color quantization.

<br><br>


<div class="texts">
<div>
<div class="w3-panel w3-teal";>
  <h2>Supervised Learning</h2>
</div>

<h3> Utilizing YOLOv3 </h3>
<h4 class="w3-text-gray"> YOLOV3 OVERVIEW AND PREPARATION </h4>
<br><br>  <div style="text-align: center;"><img style="height: 300px;" src="figure12.png" alt="" /> <p>FIGURE 12: YOLO V3 Architecture</div></p>
<br>
To briefly recap our project, we will be employing YOLOv3, which has a fast speed version of a popular object detection algorithm YOLO- You Only Look Once. YOLO v3 uses a variant of Darknet, which originally has 53 convolutional layers trained on Imagenet (hence the full name, Darknet-53). For the task of detection, 53 more layers are stacked onto it, giving us a 106 layer fully convolutional underlying architecture for YOLOv3. Also, YOLOv3 now performs multilabel classification for objects detected in images. To expand on convolutional layers, a convolutional neural network is a class of deep neural networks, most commonly applied to analyzing visual images. When programming a CNN, the input is a tensor with shape (number of images) x (image height) x (image width) x (image depth). Then after passing through a convolutional layer, the image becomes abstracted to a feature map, with shape (number of images) x (feature map height) x (feature map width) x (feature map channels). By using YOLOv3 with Darknet-53, we aim to use the 53 layers to identify even objects on a small scale in images.
<br><br>
In comparison to other object detection algorithms, YOLOv3 was known to be fast and nearly as accurate as Single Shot MultiBox (SSD). In the study “YOLOv3: An Incremental Improvement”, we can see that although the mAP was not the highest among the other methods, it was significantly faster. For the mAP, it was still high enough to be determined as accurate. Therefore, for our use and the limited time we had, we decided to move forward with YOLOv3.
<br><br>  <div style="text-align: center;"><img style="height: 300px;" src="figure13.png" alt="" /> <p>FIGURE 13: Performance of YOLO V3</div></p>
<br><br>
Since YOLOv3 originally has a set of 80 pretrained objects that it can already detect with pretrained weights, one of which is people. We want to use that as a basis and then customize the model to identify if a person is wearing a mask or if a person is not wearing a mask.
<br><br>
Furthermore, Darknet-53 is used as our feature extractor since the structure utilizes a progression of 3 * 3 and 11 convolutional layers. Compared with other backbones with the same settings (shown below), Darknet-53 performs similarly with other top class classifiers but with fewer floating point operations and faster speed. Additionally, since Darknet-53 achieves the highest measured floating point operations per second, the network structure therefore better utilizes GPU (making it more efficient at evaluating iterations).
<br><br>  <div style="text-align: center;"><img style="height: 200px;" src="figure14.png" alt="" /> <p>FIGURE 14: 1000-Class Image Net Comparison</div></p>
<br>


<h4 class="w3-text-gray"> PREPROCESSING  DATASET AND TRAINING SET CREATION</h4>
To begin our next steps, we separated our dataset into testing and training sets to begin making bounding boxes to implement YOLOv3. We collected data on both people wearing masks and people not wearing masks, and we will be training the model with parts of those data. Once the model is completed, we will test the model with test data. The results of the test model will be compared with the labeling we already made to calculate a percentage of correctness.
<br><br>
To begin YOLOv3, we divided our dataset into: (x_train, y_train) and (x_text, y_test), whereby:
<pre>
    <code>
      x-train = all of our training images
      y-train = 0, 1 labels for people with masks and without people without masks
      x-test = smaller subset of the dataset for the testing set to check accuracy of the algorithm
      y-test = smaller subset of 0, 1 for people with masks and without masks to check accuracy of algorithm
    </code>
  </pre>
  <div style="text-align: center;">
<p>FIGURE 15: Initializing testing and training images and labels</div></p>
<br>
Following this, we created text files for specifying the locations of the dataset to automate and speed up running our model on GoogleColab. We also randomly chose a few images and for each image, we manually created an associated text file to add bounding boxes for the training set. We created the files of co-bounding boxes around the training data set using labelImg. By using labelImg, the label files we need for YOLOv3 are automatically created in a YOLO-readable format. For each bounding box, we added a corresponding class label where we set the people_with_mask as 0 and people_without_mask as 1. We did run into issues with labelImg as it was difficult to install which held us back from moving forward. Additionally, we had to manually add the bounding boxes for the testing images which was time consuming. Since doing this for 60% of the image was very inefficient, we decided to create the boxes accurately for 30% of the total images for the training set. After preprocessing our dataset (i.e. creating the label files for each image), both images and their respective label files were to be kept together.
<br><br>
Bounding boxes is just a simple rectangle that is a point of reference for object detection, which essentially acts as a guideline for the algorithm to look for objects of similar size and shape. By manually creating our own bounding boxes in the training set, YOLOv3 will have a basis to annotate the images and learn to detect where the masks are. With the training data, YOLO uses k-means clustering to cluster the bounding boxes of objects to determine suitable bounding box sizes. YOLO applies a neural network to a full image. It takes an image and divides it into a grid where each cell is responsible for predicting bounding boxes. The YOLO classification layer uses three anchor boxes; thus, at each grid cell, it makes a prediction for each of three bounding boxes based on the three anchor boxes. It can also return the confidence score that indicates how likely the bounding box encloses the correct object.
<br><br>
Continuing our training preparation, we set up the necessary components to train our model by configuring YOLO with our own dataset. We first compiled our darknet repository and configured the Darknet yolo.cfg containing all of the Darknet CNN parameters. These were used for training configurations which include three yolo layers. We adjusted the number of batches, the number of convolutional kernels there are in a layer, the line filters in the convolutional layers to accommodate for our class size, and the learning rate. This also included editing our obj.names and obj.data file so that we can specify the number of classes as well as the amount of filters.
<br><br>
<h4 class="w3-text-gray"> TRAINING THE MODEL</h4>
Once we finished with the preparation, we moved forward to start training. Given that we had set up our training sets prior, we took this and ran the YOLO training command to see images resulting from data augmentation. Again, it is again important to state that the training was done on Google Colab so that we could utilize Tesla K80 GPU for a faster and more efficient training of the network. One challenge that we ran into was trying to understand the framework of YOLO V3 and applying it to create our own model. There were different compilation and runtime errors such as the bounding boxes being too small or the number of batches being too large. Additionally, running the training was extremely long where it would take up to a day to run.
<br><br>
For the training, each object in our dataset was trained for 12,000 iterations (the traditional training method is at least 2,000 iterations for each object so 3(total classes) * 2,000 = 12,000).  We next set the values of the batch and subdivisions to 32 and 8 respectively for optimal training speed. The width and height values were set at 416 for optimum speed and accuracy while the number of filters used in convolution layer were set to 21 as the value is dependent on total number of classes as, filters = (classes + 5)*3.
<br><br>  <div style="text-align: center;"><img style="height: 200px;" src="figure16.png" alt="" /> <p>FIGURE 16: The code to automatically adjust the necessary .cfg file</div></p>
In all, the total time required to train the network with the above configurations was approximately 8-10 hours. As such, we were able to generate the weights after the 12,000 iterations finished detections and analyzing the performance. As later mentioned in our challenges, this proved to be a constraint in our model as this was very time consuming (especially given the GPU limits of Google Colab).
<br><br>
Once we obtained the weights to the layers of the model, we wanted to count the amount of people wearing a mask and the people bounding boxes that it outputs using TensorFlow. To do so, we added a counter from the class names files containing all the object names for which the model is trained. For example, if the index for a person is 0 in the classes.txt file, we would check if the class predicted is 0 and if it is, we would increment the counter. The sample code we later implemented can be found below:
<br><br>
<pre>
    <code>
      #!/usr/bin/env python3
      import fileinput
      from PIL import Image
      import os, os.path

      imgs = []
      path = "/content/darknet/results"
      valid_text_files = [".txt"]
      for f in os.listdir(path):
        ext = os.path.splitext(f)[1]
        if ext.lower() not in valid_text_files:
          continue

        mask_count = 0
        without_mask_count = 0
        with fileinput.FileInput(path +"/" + f, inplace=False) as file:
          for line in file:
            if line[:17] == "people_with_masks":
              mask_count += 1
            elif line[:20] == "people_without_masks":
              without_mask_count += 1
      print('Number of people with masks:', mask_count)
      print('Number of people without masks:', without_mask_count)
    </code>
  </pre>
  <div style="text-align: center;"><p>FIGURE 17: Counter Implemented for Amount of People with Masks in Image</div></p>
Taking the results.txt file from our working model, we implemented a counter for the amount of people with masks and without masks. We adjusted our code to calculate the respective numbers as shown above and were able to output the result for each image.
<br><br>


<h4 class="w3-text-gray">DETERMINING WHEN TO STOP TRAINING</h4>
As previously mentioned, the traditional method of YOLOv3 recommends that we have at least 2,000 iterations for each class(object). This number should also not be less than the number of training images and not less than 6000 iterations in total. Hence, we ran our model for 12,000 iterations in total.
<br><br>
During our training, we also saw varying indicators of error. As such, we also stopped our training when we no longer saw significant decreases in the average loss error (avg). Theoretically, the lower the average loss, the better the model. Therefore, when the average loss no longer decreased in our many iterations, we stopped our training.
<br><br>  <div style="text-align: center;"><img style="height: 150px;" src="figure18.png" alt="" /> <p>FIGURE 18: Sample of average loss error in iteration 9002</div></p>
Next, to prevent the issue of overfitting, we also analyzed the results of our previous weights (1000,2000,..., last weights). Even though we stopped our final model at the last weights, the best result can be given in one of the earlier weights. Overfitting in our case would occur when our model detects objects on images from the training set, but no further images. As such, it is ideal to obtain weights from an earlier stopping point. In our case, we decided that the optimal weight was 4,000 after comparing the mean accuracy precision scores.
<br><br>  <div style="text-align: center;"><img style="height: 150px;" src="figure19.png" alt="" /> <p>FIGURE 19: Locating Early Stopping Point in Dataset</div></p>
We also later output a graph of our loss values as we increased our iterations. In the figure below, we mapped out the iterations to 4,000 since that was where we saw the least amount of loss and most accuracy. As observed, the loss values stayed approximately the same once the model reached 1,000 iterations.
<br><br>  <div style="text-align: center;"><img style="height: 150px;" src="figure20.png" alt="" /> <p>FIGURE 20: Training Loss chart</div></p>


<br><br>




<div class="texts">
<div>
<div class="w3-panel w3-teal";>
  <h2>RESULTS OF TRAINING</h2>
</div>

<h4 class="w3-text-gray">OVERVIEW</h4>
After we concluded our training, we tested our model against some sample pictures to evaluate its performance. We also implemented a counter onto our model to count how many people have masks and have many people who do not in a given image. The sample of some of our results are highlighted below:
<br><br>  <div style="text-align: center;"><img style="height: 250px;" src="figure21.png" alt="" /> <p>FIGURE 21: Model output of a people without masks and with masks</div></p>
We later also evaluated the performance of our model with the methods mentioned below.


<h4 class="w3-text-gray">CONFUSION MATRIX AND F-1 SCORE</h4>

A confusion matrix was utilized as a summary to give us the prediction results on our classification model. For YOLOv3, the number of correct and incorrect predictions are summarized with counted values and broken down by class. This was crucial to us since it showed the ways in which our model was confused when it makes its predictions. This allows us to gain insight into the errors being made by our classifier as well as the types of errors it is making.
<br><br>
We will make People_with_Masks as positive and People_without_Masks as negative. By using the model on the testing set and compare the results, we will be able to get the required parameters for our confusion matrix: True_Positive, False_Positive, True_Negative and False_Negative. Here’s the result that we get:

<br><br>
<pre>
    <code>
      For people with masks (ClassID = 0), TP = 227 and FP = 2.
      For people without masks (ClassID = 1), TN = 205 and FN = 9.
      Whereby:
        True positive (TP): A true positive test result is one that detects the condition when the mask is present. Our model was 227.
        True Negative (TN): A true negative test result is one that does not detect the condition when the mask is absent. Our model was 205.
        False positive (FP): A false positive test result is one that detects the condition when the mask is absent. Our model was 2.
        False Negative (FN): A false negative test result is one that does not detect the condition when the mask is present. Our model was 9.

    </code>
  </pre>
  <div style="text-align: center;"><img style="height: 250px;" src="figure22.png" alt="" /> <p>FIGURE 22: Confusion Matrix</div></p>

<br><br>
Based on the confusion matrix, we are able to compute accuracy, precision and F1-score and thus evaluate our accuracy.
<br><br> The F1 formula is as follows:
<div style="text-align: center;"><img style="height: 100px;" src="figure23.png" alt="" /> <p>FIGURE 23: F1 Formula</div></p>
where TP, FP, FN are numbers of true positives, false positives, and false negatives, respectively.
<br><br>
Plugging in the values from our model, we obtain:

<br><br><pre>
    <code>
      Accuracy = (TP + TN) / (P + N) = 0.9752
      Precision = TP / (TP + FP) = 0.9619
      F1 = 2TP / (2TP + FP + FN) = 0.9763</code>
  </pre>
  <br><br>
  The F1-score has an intuitive meaning that tells us how precise our classifier is, as well as how robust it is. Since F1 takes into calculation both accuracy and precision, if you have very low precision or recall (or both), your F-score drops lower. Since our score is 0.9763, our model has good accuracy and precision, which we also later observe with our mean accuracy precision score.


  <h4 class="w3-text-gray">EVALUATING MAP (MEAN ACCURACY PRECISION) SCORE</h4>
  The MAP metric  is the mean value of average precision for each class, where the average precision is the average value of 11 points on precision-recall curve (PR curve) for each detection. The MAP measures the performance of the model’s information retrieval and object detection abilities.
  <br><br>
  The team was able to retrieve the metrics for the model from the following command:

  <br><br><pre>
      <code>
        !./darknet detector map data/obj.data cfg/yolov3.cfg /content/weights</code>
    </pre>
    <div><p>FIGURE 24: YOLOv3 Model Evaluation Command</div></p>

    <div style="text-align: center;"><img style="height: 250px;" src="figure25.png" alt="" /> <p>FIGURE 25: Model Evaluation Results</div></p>

    The results showed that the accuracy precision of our model for class 0, people_with_masks, is 100% with 227 true positive and 2 false positive detections. For class 1, people_without_masks, the accuracy precision is 99.87% with 205 true positive and 9 false positive detections. The MAP of this model is then calculated to be 99.93% with Intersection Over Union (IoU) of 0.5.  The model has an average IoU of 83.9% meaning the model is true positive overall.

  <div style="text-align: center;"><img style="height: 250px;" src="figure26.png" alt="" /> <p>FIGURE 26: Explanation of the Calculation of IoU</div></p>

  <br><br>
  To train an object detection model, there are usually two inputs:
  <li>An image.</li>
  <li>
  Ground-truth bounding boxes for each object within the image.</li>
  <br>
Since it is difficult to subjectively evaluate the model predictions based on the visualization of the 2 boxes, we use IoU as a quantitative measure to score how the ground-truth and predicted boxes match. In doing so, we know if a region has an object or not (as seen in the figure above). The higher the IoU, the better the prediction. From our model, an IoU score of 0.839 means there is a 83.9% overlap between the ground-truth and predicted bounding boxes which indicates that the model yields reasonably “good predictions”.
<br>
<div class="texts">
<div>
<div class="w3-panel w3-teal";>
  <h2>Challenges</h2>
</div>
Some minor issues that we have faced include the runtime of Google colab’s GPU. We find that since our dataset is so large, that along with YOLOv3, rerunning Colab every time can be very time consuming. This is especially since we have to download the weights of YOLOv3 to train our model. Not only that, GoogleColab refreshes so we have to find a way to not have to manually load it every time; our solution was to use Kaggle, which automatically updates in connection to our GitHub page. Doing helped mitigate some of this time.

<br><br>
Another challenge we faced was finding a way to compress our images small enough, but still be readable and maintain the accuracy of the model. We considered various techniques to apply additional unsupervised learning techniques to our project.
<br><br>
Additionally, we attempted k-means clustering and the confusion matrix on our training and testing data set in order to explore more about our model and optimize the algorithm. However, we encountered the problem of our images being different sizes (since they were exported from Google Images), so we had difficulty resizing the images into a standard array for normalization.
<br><br>
Moving onto preparation for the YOLO training, we had to manually add the bounding boxes for 30% of the images which was extremely time-consuming and mentally tiring. Ideally, we would have liked to have 60% of the images to be a part of our training set to make our model more accurate. In the future, to expand our model further, we could add more images and their bounding boxes to the training set or look into an alternate method to quickly add these.
<br><br>
While we were training our data, we continuously ran into issues with saving the resulting weights of our training. Although we had tested YOLO with a few images and the weights were able to successfully save, the weights would not save properly with our larger training set. We would only discover the issue after the training had run for hours. Additionally, Google Colab had a limited GPU, so we could only run a certain amount each day and run the rest the next.
After addressing these errors, we were able to successfully train our model.
<br><br>
In conclusion, we ran into several challenges along the way including time-consuming work and runtime issues; however, we were able to overcome this and created a working mask identification model. In the future, we would recommend implementing this on a server with more GPU and allocating more time for training since that was where most of our issues were. As well, this model could be further expanded with video as YOLOv3 is compatible in detecting objects in a video.













</div>
</div>
<br><br>
<h3>References</h3>
  <ul style="font-style: italic;">
    <li>
      https://arxiv.org/pdf/1612.08242.pdf
    </li>
    <li>
      https://jonathan-hui.medium.com/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo-5425656ae359
    </li>
    <li>
      https://www.motorolasolutions.com/content/dam/msi/docs/healthresources/face-mask-detection-white-paper.pdf
    </li>
    <li>
      Redmon, Joseph, and Ali Farhadi. YOLOv3: An Incremental Improvement. pjreddie.com/media/files/papers/YOLOv3.pdf.
    </li>
    <li>
      Rosebrock, Adrian. “COVID-19: Face Mask Detector with OpenCV, Keras/TensorFlow, and Deep Learning.” PyImageSearch, 4 May 2020, www.pyimagesearch.com/2020/05/04/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/.
    </li>
    <li>
      Xin, M., Wang, Y. Research on image classification model based on deep convolution neural network. J Image Video Proc. 2019, 40 (2019). https://doi.org/10.1186/s13640-019-0417-8
    </li>
  </ul>
  <hr>
  <footer>
  <p>© Nina Chen, Sabrina Chua, Siyu Li, Yuhe Chen</p>
  </footer>
</div>
</div>
</div>
<br><br>


</body></html>
